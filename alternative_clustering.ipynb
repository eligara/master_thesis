{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import os, sys\n",
    "from hsbmpy import get_file, define_labels, get_cluster_given_l\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 2\n",
    "#setup = 'oversigma_10tissue'\n",
    "#label = 'disease_type'\n",
    "label='primary_site'\n",
    "#label = 'uniq'\n",
    "L = 4\n",
    "#labels = ['primary_site', 'disease_type']\n",
    "#labels = ['primary_site', 'secondary_site']\n",
    "#directory = \"/Users/filippo/Google Drive File Stream/My Drive/tesi_magistrale/tesi/results/hSBM/highlyvariable_10tissue\"\n",
    "directory=r\"/home/fvalle/phd/results/tcga/oversampling_10tissue\"\n",
    "directory=r\"/home/fvalle/phd/results/altmann\"\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3140 entries, a to zgb\n",
      "Data columns (total 63 columns):\n",
      "De_novo_transcriptome_assembly                                            3140 non-null int64\n",
      "BioUML                                                                    3140 non-null int64\n",
      "Knotted_protein                                                           3140 non-null int64\n",
      "IEEE/ACM_Transactions_on_Computational_Biology_and_Bioinformatics         3140 non-null int64\n",
      "Aureus_Sciences                                                           3140 non-null int64\n",
      "Sepp_Hochreiter                                                           3140 non-null int64\n",
      "SnoRNA_prediction_software                                                3140 non-null int64\n",
      "Enzyme_Function_Initiative                                                3140 non-null int64\n",
      "Law_of_Maximum                                                            3140 non-null int64\n",
      "Louis_and_Beatrice_Laufer_Center_for_Physical_and_Quantitative_Biology    3140 non-null int64\n",
      "International_Society_for_Computational_Biology                           3140 non-null int64\n",
      "Premier_Biosoft                                                           3140 non-null int64\n",
      "Foldit                                                                    3140 non-null int64\n",
      "Journal_of_Computational_Biology                                          3140 non-null int64\n",
      "K-mer                                                                     3140 non-null int64\n",
      "Folding@home                                                              3140 non-null int64\n",
      "Computational_biology                                                     3140 non-null int64\n",
      "Bioinformatics                                                            3140 non-null int64\n",
      "Quantum_oscillations_(experimental_technique)                             3140 non-null int64\n",
      "Holometer                                                                 3140 non-null int64\n",
      "Euler's_laws_of_motion                                                    3140 non-null int64\n",
      "Dynamic_mode_decomposition                                                3140 non-null int64\n",
      "Fragment_separator                                                        3140 non-null int64\n",
      "List_of_Directors_General_of_CERN                                         3140 non-null int64\n",
      "Einstein\\xe2\\x80\\x93de_Haas_effect                                        3140 non-null int64\n",
      "Point_source                                                              3140 non-null int64\n",
      "X-ray_standing_waves                                                      3140 non-null int64\n",
      "Line_source                                                               3140 non-null int64\n",
      "Faraday_cup_electrometer                                                  3140 non-null int64\n",
      "X-ray_crystal_truncation_rod                                              3140 non-null int64\n",
      "Philosophical_interpretation_of_classical_physics                         3140 non-null int64\n",
      "Wave_tank                                                                 3140 non-null int64\n",
      "Elevator_paradox_(physics)                                                3140 non-null int64\n",
      "Complementary_experiments                                                 3140 non-null int64\n",
      "Experimental_physics                                                      3140 non-null int64\n",
      "Particle-induced_X-ray_emission                                           3140 non-null int64\n",
      "Ripple_tank                                                               3140 non-null int64\n",
      "Uncertainty                                                               3140 non-null int64\n",
      "Newton's_laws_of_motion                                                   3140 non-null int64\n",
      "SLAC_National_Accelerator_Laboratory                                      3140 non-null int64\n",
      "Pauli_effect                                                              3140 non-null int64\n",
      "Empirical_formula                                                         3140 non-null int64\n",
      "Ziff-Gulari-Barshad_model                                                 3140 non-null int64\n",
      "McConnell_equation                                                        3140 non-null int64\n",
      "Molecular_beam                                                            3140 non-null int64\n",
      "Photofragment-ion_imaging                                                 3140 non-null int64\n",
      "Reactive_empirical_bond_order                                             3140 non-null int64\n",
      "Magic_angle_(EELS)                                                        3140 non-null int64\n",
      "Electrostatic_deflection_(structural_element)                             3140 non-null int64\n",
      "Fuel_mass_fraction                                                        3140 non-null int64\n",
      "Molecular_vibration                                                       3140 non-null int64\n",
      "RRKM_theory                                                               3140 non-null int64\n",
      "Rotating_wave_approximation                                               3140 non-null int64\n",
      "Anisotropic_liquid                                                        3140 non-null int64\n",
      "Polarizability                                                            3140 non-null int64\n",
      "Knight_shift                                                              3140 non-null int64\n",
      "Dynamic_nuclear_polarisation                                              3140 non-null int64\n",
      "Rotational_transition                                                     3140 non-null int64\n",
      "Chemical_physics                                                          3140 non-null int64\n",
      "Effective_field_theory                                                    3140 non-null int64\n",
      "Rovibrational_coupling                                                    3140 non-null int64\n",
      "Quantum_solvent                                                           3140 non-null int64\n",
      "Nuclear_Overhauser_effect                                                 3140 non-null int64\n",
      "dtypes: int64(63)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mainTable.csv\", index_col=[0], header=[0])\n",
    "totalobjcets = len(df.columns)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 63 entries, De_novo_transcriptome_assembly to Nuclear_Overhauser_effect\n",
      "Data columns (total 1 columns):\n",
      "primary_site    63 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 1008.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_files = pd.read_csv(\"files.dat\", index_col=[0])\n",
    "df_files.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'>\n"
     ]
    }
   ],
   "source": [
    "true_out = []\n",
    "for sample in df.columns.values:\n",
    "    try:\n",
    "        true_out.append(get_file(sample, df_files)['primary_site'])\n",
    "    except:\n",
    "        print(sys.exc_info()[0])\n",
    "        true_out.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clustersizes.txt\",'r') as f:\n",
    "    xl = np.array(f.read().split(sep='\\n'))[:-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(793, 3739)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hierarchical\n",
      "testing with 793 clusters\n",
      "saving clusters\n",
      "testing with 137 clusters\n",
      "saving clusters\n",
      "testing with 17 clusters\n",
      "saving clusters\n",
      "testing with 6 clusters\n",
      "saving clusters\n",
      "testing with 1 clusters\n",
      "saving clusters\n"
     ]
    }
   ],
   "source": [
    "fig=plt.figure()\n",
    "dend = shc.dendrogram(shc.linkage(df.T.values, method='ward'), leaf_rotation=90., leaf_font_size=8.,)\n",
    "plt.xlabel(\"samples\", fontsize=16)\n",
    "plt.show()\n",
    "fig.savefig(\"hierarchical_dendogram.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hierarchical\n",
      "testing with 793 clusters\n",
      "saving clusters\n",
      "testing with 137 clusters\n",
      "saving clusters\n",
      "testing with 17 clusters\n",
      "saving clusters\n",
      "testing with 6 clusters\n",
      "saving clusters\n",
      "testing with 1 clusters\n",
      "saving clusters\n"
     ]
    }
   ],
   "source": [
    "#hierarchical\n",
    "scores['hierarchical']={\n",
    "    'h':[],\n",
    "    'c':[],\n",
    "    'V':[]\n",
    "}\n",
    "print(\"hierarchical\")\n",
    "os.system('mkdir -p hierarchical')\n",
    "hierarchical_model = AgglomerativeClustering(n_clusters=1, affinity='euclidean', linkage='complete')  \n",
    "for l,x in enumerate(xl):\n",
    "    print(\"testing with %d clusters\"%x)\n",
    "    hierarchical_model.n_clusters=x\n",
    "    out = hierarchical_model.fit_predict(df.T.values)\n",
    "        \n",
    "    #save clusters\n",
    "    print(\"saving clusters\")\n",
    "    df_clusters = pd.DataFrame(index=np.arange(totalobjcets))\n",
    "    for c in np.arange(out.max()+1)[::-1]:\n",
    "        c_objects = df.columns[np.argwhere(out==c)].values.T[0]\n",
    "        df_clusters.insert(0,\"Cluster %d\"%(c+1),np.concatenate((c_objects,[np.nan for _ in np.arange(totalobjcets-len(c_objects))])))\n",
    "    df_clusters.dropna(axis=0,how='all', inplace=True)\n",
    "    df_clusters.to_csv(\"hierarchical/hierarchical_level_%d_clusters.csv\"%(l), index=False, header=True)\n",
    "    \n",
    "    score = (homogeneity_completeness_v_measure(true_out, out))\n",
    "    scores['hierarchical']['h'].append(score[0])\n",
    "    scores['hierarchical']['c'].append(score[1])\n",
    "    scores['hierarchical']['V'].append(score[2])\n",
    "    \n",
    "pd.DataFrame(data=scores['hierarchical']).to_csv(\"hierarchical.scores\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topicsizes.txt\",'r') as f:\n",
    "    tl = np.array(f.read().split(sep='\\n'))[:-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = [10,50,100,500]\n",
    "tl = [10, 50, 100,500]\n",
    "Sigmas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda\n",
      "testing with 10 clusters and 10 topics\n",
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=1, evaluate_every=-1,\n",
      "                          learning_decay=0.7, learning_method='online',\n",
      "                          learning_offset=10.0, max_doc_update_iter=5,\n",
      "                          max_iter=5, mean_change_tol=0.001, n_components=10,\n",
      "                          n_jobs=1, perp_tol=0.1, random_state=42,\n",
      "                          topic_word_prior=1, total_samples=1000000.0,\n",
      "                          verbose=2)\n",
      "iteration: 1 of max_iter: 5\n",
      "iteration: 2 of max_iter: 5\n",
      "iteration: 3 of max_iter: 5\n",
      "iteration: 4 of max_iter: 5\n",
      "iteration: 5 of max_iter: 5\n",
      "saving word-distr\n",
      "saving topic-distr\n",
      "saving clusters\n",
      "saving metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing with 50 clusters and 50 topics\n",
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=1, evaluate_every=-1,\n",
      "                          learning_decay=0.7, learning_method='online',\n",
      "                          learning_offset=10.0, max_doc_update_iter=5,\n",
      "                          max_iter=5, mean_change_tol=0.001, n_components=50,\n",
      "                          n_jobs=1, perp_tol=0.1, random_state=42,\n",
      "                          topic_word_prior=1, total_samples=1000000.0,\n",
      "                          verbose=2)\n",
      "iteration: 1 of max_iter: 5\n",
      "iteration: 2 of max_iter: 5\n",
      "iteration: 3 of max_iter: 5\n",
      "iteration: 4 of max_iter: 5\n",
      "iteration: 5 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving word-distr\n",
      "saving topic-distr\n",
      "saving clusters\n",
      "saving metrics\n",
      "testing with 100 clusters and 100 topics\n",
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=1, evaluate_every=-1,\n",
      "                          learning_decay=0.7, learning_method='online',\n",
      "                          learning_offset=10.0, max_doc_update_iter=5,\n",
      "                          max_iter=5, mean_change_tol=0.001, n_components=100,\n",
      "                          n_jobs=1, perp_tol=0.1, random_state=42,\n",
      "                          topic_word_prior=1, total_samples=1000000.0,\n",
      "                          verbose=2)\n",
      "iteration: 1 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2 of max_iter: 5\n",
      "iteration: 3 of max_iter: 5\n",
      "iteration: 4 of max_iter: 5\n",
      "iteration: 5 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving word-distr\n",
      "saving topic-distr\n",
      "saving clusters\n",
      "saving metrics\n",
      "testing with 500 clusters and 500 topics\n",
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=1, evaluate_every=-1,\n",
      "                          learning_decay=0.7, learning_method='online',\n",
      "                          learning_offset=10.0, max_doc_update_iter=5,\n",
      "                          max_iter=5, mean_change_tol=0.001, n_components=500,\n",
      "                          n_jobs=1, perp_tol=0.1, random_state=42,\n",
      "                          topic_word_prior=1, total_samples=1000000.0,\n",
      "                          verbose=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 5 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving word-distr\n",
      "saving topic-distr\n",
      "saving clusters\n",
      "saving metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "scores['lda']={\n",
    "    'h':[],\n",
    "    'c':[],\n",
    "    'V':[]\n",
    "}\n",
    "print(\"lda\")\n",
    "os.system('mkdir -p lda')\n",
    "lda = LatentDirichletAllocation(n_components=1, \n",
    "                                    random_state=42, \n",
    "                                    n_jobs=1, \n",
    "                                    learning_method='online', \n",
    "                                    max_doc_update_iter=5,\n",
    "                                    max_iter=5,\n",
    "                                    topic_word_prior=1,\n",
    "                                    doc_topic_prior=1,\n",
    "                                    verbose=2)\n",
    "for l,x in enumerate(xl):\n",
    "    #lda\n",
    "    ntopic = tl[l]\n",
    "    #ntopic = x\n",
    "    print(\"testing with %d clusters and %d topics\"%(x,ntopic))\n",
    "    lda.n_components=ntopic\n",
    "    print(lda)\n",
    "    topics = lda.fit_transform(df.T.values)\n",
    "    \n",
    "    #save word distr\n",
    "    print(\"saving word-distr\")\n",
    "    df_word_distr = pd.DataFrame(data=lda.components_.T, index=df.index, columns=[\"Topic %d\"%(t+1) for t in np.arange(ntopic)])\n",
    "    df_word_distr.to_csv(\"lda/lda_level_%d_word-dist.csv\"%l, index=True, header=True)\n",
    "    \n",
    "    #save topic distr\n",
    "    print(\"saving topic-distr\")\n",
    "    df_topic_distr = pd.DataFrame(data=topics, columns=[\"Topic %d\"%(t+1) for t in np.arange(ntopic)])\n",
    "    df_topic_distr.insert(0,'i_doc',np.arange(len(df.columns)))\n",
    "    df_topic_distr.insert(1,'doc',df.columns)\n",
    "    df_topic_distr.to_csv(\"lda/lda_level_%d_topic-dist.csv\"%l, index=False, header=True)\n",
    "    \n",
    "    #save clusters\n",
    "    print(\"saving clusters\")\n",
    "    df_clusters = pd.DataFrame(index=np.arange(totalobjcets))\n",
    "    #cluster = AgglomerativeClustering(n_clusters=x, affinity='euclidean', linkage='ward')  \n",
    "    #out = cluster.fit_predict(topics)\n",
    "    out = np.argmax(topics, axis=1)\n",
    "    \n",
    "    for c in np.arange(out.max()+1)[::-1]:\n",
    "        c_objects = df.columns[np.argwhere(out==c)].values.T[0]\n",
    "        df_clusters.insert(0,\"Cluster %d\"%(c+1),np.concatenate((c_objects,[np.nan for _ in np.arange(totalobjcets-len(c_objects))])))\n",
    "    df_clusters.dropna(axis=0,how='all', inplace=True)\n",
    "    df_clusters.to_csv(\"lda/lda_level_%d_clusters.csv\"%(l), index=False, header=True)\n",
    "    \n",
    "    #metrics\n",
    "    print(\"saving metrics\")\n",
    "    score = (homogeneity_completeness_v_measure(true_out, out))\n",
    "    scores['lda']['h'].append(score[0])\n",
    "    scores['lda']['c'].append(score[1])\n",
    "    scores['lda']['V'].append(score[2])\n",
    "    \n",
    "    #save dl\n",
    "    Sigmas.append(-lda.score(df.values.T))\n",
    "    \n",
    "pd.DataFrame(data=scores['lda']).to_csv(\"%s/lda.scores\"%directory, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[277209.87360436836, 286471.42312461976, 288577.7857844125, 342245.58609012153]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4375063841949788, 0.7449935483935977, 0.5512718007395827)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homogeneity_completeness_v_measure(true_out, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hierachical Altmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiermodel = AgglomerativeClustering(n_clusters=10, affinity='euclidean', linkage='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clustersizes.txt') as f:\n",
    "    xl=np.array(f.read().split('\\n')[:-1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mkdir -p hierhsbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_out = []\n",
    "for sample in pd.read_csv(\"%s/%s_level_%d_topic-dist.csv\"%('topsbm','topsbm',0), index_col=1).drop('i_doc', axis=1).index.values:\n",
    "    try:\n",
    "        true_out.append(get_file(sample, df_files)['primary_site'])\n",
    "    except:\n",
    "        print(sys.exc_info()[0])\n",
    "        true_out.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['hierhsbm']={\n",
    "    'h':[],\n",
    "    'c':[],\n",
    "    'V':[]\n",
    "}\n",
    "for l,n_clusters in enumerate(xl):\n",
    "    print(\"Fitting level %d with %d clusters\"%(l, n_clusters))\n",
    "    df_topics = pd.read_csv(\"%s/%s_level_%d_topic-dist.csv\"%('topsbm','topsbm',l), index_col=1).drop('i_doc', axis=1)\n",
    "    df_clusters = pd.DataFrame(columns=[\"Cluster %d\"%c for c in np.arange(n_clusters)+1])\n",
    "    hiermodel.n_clusters=n_clusters\n",
    "    out = hiermodel.fit_predict(df_topics.values)  \n",
    "    for c in np.arange(out.max()+1)[::-1]:\n",
    "        c_objects = df_topics.index[np.argwhere(out==c)].values.T[0]\n",
    "        df_clusters[\"Cluster %d\"%(c+1)]=np.concatenate((c_objects,[np.nan for _ in np.arange(len(df_topics.index)-len(c_objects))]))\n",
    "    df_clusters.dropna(axis=0,how='all', inplace=True)\n",
    "    df_clusters.to_csv(\"hierhsbm/hierhsbm_level_%d_clusters.csv\"%(l), index=False, header=True)\n",
    "    #metrics\n",
    "    print(\"saving metrics\")\n",
    "    score = (homogeneity_completeness_v_measure(true_out, out))\n",
    "    scores['hierhsbm']['h'].append(score[0])\n",
    "    scores['hierhsbm']['c'].append(score[1])\n",
    "    scores['hierhsbm']['V'].append(score[2])\n",
    "    \n",
    "pd.DataFrame(data=scores['hierhsbm']).to_csv(\"%s/hierhsbm.scores\"%directory, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
