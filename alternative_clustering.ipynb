{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fvalle/anaconda3/envs/phd/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import os, sys\n",
    "from hsbmpy import get_file, define_labels, get_cluster_given_l, get_max_available_L\n",
    "from geneontology import topic_analysis\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from lda import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/fvalle/phd/master_thesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "level = 2\n",
    "#setup = 'oversigma_10tissue'\n",
    "#label = 'disease_type'\n",
    "label='lda'\n",
    "#label = 'uniq'\n",
    "#labels = ['primary_site', 'disease_type']\n",
    "#labels = ['primary_site', 'secondary_site']\n",
    "directory=r\"/home/fvalle/phd/datasets/breast_HDE\"\n",
    "#L=get_max_available_L(directory)\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15991 entries, ENSG00000186160 to ENSG00000181458\n",
      "Columns: 197 entries, 22c16f46-950f-4389-bb16-b5cec4d8b94e.FPKM.txt.gz to 67943973-e57a-4285-bd6e-995053d23fe0.FPKM.txt.gz\n",
      "dtypes: int64(197)\n",
      "memory usage: 24.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"mainTable.csv\", index_col=[0], header=[0]).dropna().astype(int)\n",
    "totalobjcets = len(df.columns)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1205 entries, fa9d8ef2-0fd0-4fd8-8266-e4760b77d103.FPKM.txt.gz to 110de223-52e1-43b6-8c69-399a0af8b174.FPKM.txt.gz\n",
      "Data columns (total 6 columns):\n",
      "cases.0.project.primary_site    1205 non-null object\n",
      "cases.0.submitter_id            1205 non-null object\n",
      "cases.0.project.disease_type    1205 non-null object\n",
      "cancer.type                     1205 non-null object\n",
      "Subtype_mRNA                    1205 non-null object\n",
      "Subtype_Selected                1205 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 65.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_files = pd.read_csv(\"files.dat\", index_col=[0])\n",
    "df_files.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_out = []\n",
    "for sample in df.columns.values:\n",
    "    try:\n",
    "        true_out.append(get_file(sample, df_files)['Subtype_Selected'])\n",
    "    except:\n",
    "        print(sys.exc_info()[1])\n",
    "        true_out.append('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20,  2,  1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"clustersizes.txt\",'r') as f:\n",
    "        xl = np.array(f.read().split(sep='\\n'))[:-1].astype(int)\n",
    "except:\n",
    "        xl=np.linspace(2,50,5, dtype=int)\n",
    "xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197, 15991)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "dend = shc.dendrogram(shc.linkage(df.T.values, method='ward'), leaf_rotation=90., leaf_font_size=8.,)\n",
    "plt.xlabel(\"samples\", fontsize=16)\n",
    "plt.show()\n",
    "fig.savefig(\"hierarchical_dendogram.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hierarchical\n",
    "scores['hierarchical']={\n",
    "    'h':[],\n",
    "    'c':[],\n",
    "    'V':[]\n",
    "}\n",
    "print(\"hierarchical\")\n",
    "os.system('mkdir -p hierarchical')\n",
    "hierarchical_model = AgglomerativeClustering(n_clusters=1, affinity='euclidean', linkage='complete')  \n",
    "for l,x in enumerate(xl):\n",
    "    print(\"testing with %d clusters\"%x)\n",
    "    hierarchical_model.n_clusters=x\n",
    "    out = hierarchical_model.fit_predict(df.T.values)\n",
    "        \n",
    "    #save clusters\n",
    "    print(\"saving clusters\")\n",
    "    df_clusters = pd.DataFrame(index=np.arange(totalobjcets))\n",
    "    for c in np.arange(out.max()+1)[::-1]:\n",
    "        c_objects = df.columns[np.argwhere(out==c)].values.T[0]\n",
    "        df_clusters.insert(0,\"Cluster %d\"%(c+1),np.concatenate((c_objects,[np.nan for _ in np.arange(totalobjcets-len(c_objects))])))\n",
    "    df_clusters.dropna(axis=0,how='all', inplace=True)\n",
    "    df_clusters.to_csv(\"hierarchical/hierarchical_level_%d_clusters.csv\"%(l), index=False, header=True)\n",
    "    \n",
    "    score = (homogeneity_completeness_v_measure(true_out, out))\n",
    "    scores['hierarchical']['h'].append(score[0])\n",
    "    scores['hierarchical']['c'].append(score[1])\n",
    "    scores['hierarchical']['V'].append(score[2])\n",
    "    \n",
    "pd.DataFrame(data=scores['hierarchical']).to_csv(\"hierarchical.scores\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"topicsizes.txt\",'r') as f:\n",
    "    tl = np.array(f.read().split(sep='\\n'))[:-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f1d5086c567c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xl' is not defined"
     ]
    }
   ],
   "source": [
    "print(xl, tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xl = xl[2:]\n",
    "tl = xl\n",
    "Sigmas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15991 entries, ENSG00000186160 to ENSG00000181458\n",
      "Columns: 197 entries, 22c16f46-950f-4389-bb16-b5cec4d8b94e.FPKM.txt.gz to 67943973-e57a-4285-bd6e-995053d23fe0.FPKM.txt.gz\n",
      "dtypes: int64(197)\n",
      "memory usage: 24.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1205 entries, fa9d8ef2-0fd0-4fd8-8266-e4760b77d103.FPKM.txt.gz to 110de223-52e1-43b6-8c69-399a0af8b174.FPKM.txt.gz\n",
      "Data columns (total 6 columns):\n",
      "cases.0.project.primary_site    1205 non-null object\n",
      "cases.0.submitter_id            1205 non-null object\n",
      "cases.0.project.disease_type    1205 non-null object\n",
      "cancer.type                     1205 non-null object\n",
      "Subtype_mRNA                    1205 non-null object\n",
      "Subtype_Selected                1205 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 65.9+ KB\n",
      "None\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1205 entries, fa9d8ef2-0fd0-4fd8-8266-e4760b77d103.FPKM.txt.gz to 110de223-52e1-43b6-8c69-399a0af8b174.FPKM.txt.gz\n",
      "Data columns (total 6 columns):\n",
      "cases.0.project.primary_site    1205 non-null object\n",
      "cases.0.submitter_id            1205 non-null object\n",
      "cases.0.project.disease_type    1205 non-null object\n",
      "cancer.type                     1205 non-null object\n",
      "Subtype_mRNA                    1205 non-null object\n",
      "Subtype_Selected                1205 non-null object\n",
      "dtypes: object(6)\n",
      "memory usage: 105.9+ KB\n",
      "None\n",
      "lda\n",
      "testing with 20 clusters and 20 topics\n",
      "lda(doc_topic_prior=1, learning_method='online', max_doc_update_iter=5,\n",
      "    max_iter=5, random_state=42, topic_word_prior=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 5 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving word-distr\n",
      "saving topic-distr\n",
      "WARNING:tensorflow:From /home/fvalle/phd/master_thesis/lda.py:108: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/fvalle/phd/master_thesis/lda.py:116: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "saving clusters\n",
      "saving metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing with 2 clusters and 2 topics\n",
      "lda(doc_topic_prior=1, learning_method='online', max_doc_update_iter=5,\n",
      "    max_iter=5, random_state=42, topic_word_prior=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 5 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving word-distr\n",
      "saving topic-distr\n",
      "saving clusters\n",
      "saving metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing with 1 clusters and 1 topics\n",
      "lda(doc_topic_prior=1, learning_method='online', max_doc_update_iter=5,\n",
      "    max_iter=5, random_state=42, topic_word_prior=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 5 of max_iter: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving word-distr\n",
      "saving topic-distr\n",
      "saving clusters\n",
      "saving metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "model=lda(n_jobs=1, verbose=2)\n",
    "model.full_analysis(directory, xl,tl=None, label='Subtype_Selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(figsize=(20,10))\n",
    "for topic in df_D.columns:\n",
    "    df_D[topic].hist(ax=ax, histtype='step', density=True, lw=2, label=topic)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('$D^g[k]$', fontsize=20)\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.tick_params(axis='x', rotation=60)\n",
    "ax.legend(ncol=5)\n",
    "plt.show()\n",
    "fig.savefig(\"%s/distinctivness.pdf\"%directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_analysis(directory,1, 'lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "homogeneity_completeness_v_measure(true_out, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.corpora import MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus=list(map(list,map(lambda x: list(zip(range(len(df.index)), df[x])),df.columns)))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dictionary=Dictionary([df.index])\n",
    "[a for a in dictionary.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_file=get_tmpfile(\"/home/fvalle/phd/datasets/tcga/oversampling_10tissue/corpus.mm\")\n",
    "MmCorpus.serialize(out_file, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LdaMallet(\"/home/fvalle/phd/Mallet/bin/mallet\", workers=5, corpus=corpus, num_topics=15, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics=pd.DataFrame(data=model.get_topics().T, index=[a[1] for a in dictionary.items()], columns=[\"Topic %d\"%(t+1) for t in range(5)])\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topic_distr=pd.read_csv(model.fdoctopics(), sep='\\t', header=None, index_col=0).drop(1,1)\n",
    "df_topic_distr.columns=[\"Topic %d\"%(t+1) for t in range(5)]\n",
    "df_topic_distr.index=df.columns\n",
    "df_topic_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topic_distr.apply(lambda x: x.idxmax().split(\" \")[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hierachical on Altmann's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hiermodel = AgglomerativeClustering(n_clusters=10, affinity='euclidean', linkage='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('clustersizes.txt') as f:\n",
    "    xl=np.array(f.read().split('\\n')[:-1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.system(\"mkdir -p hierhsbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_out = []\n",
    "for sample in pd.read_csv(\"%s/%s_level_%d_topic-dist.csv\"%('topsbm','topsbm',0), index_col=1).drop('i_doc', axis=1).index.values:\n",
    "    try:\n",
    "        true_out.append(get_file(sample, df_files)['primary_site'])\n",
    "    except:\n",
    "        print(sys.exc_info()[0])\n",
    "        true_out.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores['hierhsbm']={\n",
    "    'h':[],\n",
    "    'c':[],\n",
    "    'V':[]\n",
    "}\n",
    "for l,n_clusters in enumerate(xl):\n",
    "    print(\"Fitting level %d with %d clusters\"%(l, n_clusters))\n",
    "    df_topics = pd.read_csv(\"%s/%s_level_%d_topic-dist.csv\"%('topsbm','topsbm',l), index_col=1).drop('i_doc', axis=1)\n",
    "    df_clusters = pd.DataFrame(columns=[\"Cluster %d\"%c for c in np.arange(n_clusters)+1])\n",
    "    hiermodel.n_clusters=n_clusters\n",
    "    out = hiermodel.fit_predict(df_topics.values)  \n",
    "    for c in np.arange(out.max()+1)[::-1]:\n",
    "        c_objects = df_topics.index[np.argwhere(out==c)].values.T[0]\n",
    "        df_clusters[\"Cluster %d\"%(c+1)]=np.concatenate((c_objects,[np.nan for _ in np.arange(len(df_topics.index)-len(c_objects))]))\n",
    "    df_clusters.dropna(axis=0,how='all', inplace=True)\n",
    "    df_clusters.to_csv(\"hierhsbm/hierhsbm_level_%d_clusters.csv\"%(l), index=False, header=True)\n",
    "    #metrics\n",
    "    print(\"saving metrics\")\n",
    "    score = (homogeneity_completeness_v_measure(true_out, out))\n",
    "    scores['hierhsbm']['h'].append(score[0])\n",
    "    scores['hierhsbm']['c'].append(score[1])\n",
    "    scores['hierhsbm']['V'].append(score[2])\n",
    "    \n",
    "pd.DataFrame(data=scores['hierhsbm']).to_csv(\"%s/hierhsbm.scores\"%directory, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
