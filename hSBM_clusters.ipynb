{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "import numpy as np\n",
    "from textwrap import wrap\n",
    "from matplotlib import pyplot as plt\n",
    "from TCGA_files import *\n",
    "#from ensembleAPI import geneinfo, genesinfo\n",
    "from IPython.display import HTML\n",
    "from sklearn import metrics\n",
    "from hsbmpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, hsbmpy\n",
    "importlib.reload(hsbmpy)\n",
    "from hsbmpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 1\n",
    "#setup='highlyvariable_colonrectal'\n",
    "#setup = 'oversigma_10tissue'\n",
    "#label = 'disease_type'\n",
    "label='primary_site'\n",
    "L = 4\n",
    "labels = ['primary_site', 'disease_type', 'disease_tissue']\n",
    "#labels = ['primary_site', 'secondary_site']\n",
    "algorithm = 'topsbm'\n",
    "#labels = ['primary_site', 'secondary_site', 'status']\n",
    "#labels=['RPPA Clusters']\n",
    "#directory = \"/home/fvalle/phd/results/tcga/oversampling_10tissue\"\n",
    "directory = \"/Users/filippo/Google Drive File Stream/My Drive/tesi_magistrale/tesi/results/hSBM/highlyvariable_10tissue/\"\n",
    "#directory = \"/Users/filippo/Google Drive File Stream/My Drive/tesi_magistrale/tesi/gtex/hsbm/oversigma_10tissue/\"\n",
    "df_clusters = pd.read_csv(\"%s/%s/%s_level_%d_clusters.csv\"%(directory,algorithm,algorithm,L), header=[0])\n",
    "df_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = pd.read_csv(\"%s/files.dat\"%directory, index_col=[0], header=[0])\n",
    "#df_files.insert(2,'disease_tissue',[\"%s[%s]\"%(df_files.at[s,'primary_site'],df_files.at[s,'disease_type']) for s in df_files.index])\n",
    "labels=df_files.columns\n",
    "#df_files.groupby(['primary_site','disease_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = get_cluster_given_l(0, directory,algorithm=algorithm)\n",
    "fraction_sites_shuffle = get_fraction_sites(cluster, pd.read_csv(\"%s/files.dat.shuf\"%directory, index_col=[0]),label=label, normalise=normalise)\n",
    "clustersinfo_shuffle = get_clustersinfo(cluster, fraction_sites_shuffle)\n",
    "plot_maximum(clustersinfo,cluster,label,2, directory,algorithm=algorithm, clustersinfo_shuffle=clustersinfo_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for normalise in [True, False]:\n",
    "    for label in labels:\n",
    "        for level in np.arange(L+1)[::-1]:\n",
    "            if level==0:\n",
    "                #pass\n",
    "                continue\n",
    "            print(normalise, label, level)\n",
    "            try:\n",
    "                cluster = get_cluster_given_l(level, directory,algorithm=algorithm)\n",
    "                fraction_sites = get_fraction_sites(cluster,df_files=df_files,label=label, normalise=normalise)\n",
    "\n",
    "                #fsdf = pd.DataFrame(data=fraction_sites)\n",
    "                #fsdf = fsdf.drop('Other', axis=1)\n",
    "                #fsdf = fsdf.divide(fsdf.sum(axis=1), axis=0).fillna(0)\n",
    "                #fraction_sites = fsdf.sort_values(by=fsdf.columns.to_list(), ascending=True).to_dict(orient='list')\n",
    "\n",
    "                clustersinfo = get_clustersinfo(cluster,fraction_sites)\n",
    "                plot_cluster_composition(fraction_sites,directory,level,label=label, normalise=normalise,algorithm=algorithm)\n",
    "                make_heatmap(fraction_sites, directory, label, level, normalise=normalise,algorithm=algorithm)\n",
    "\n",
    "                clustersinfo = get_clustersinfo(cluster,fraction_sites)            \n",
    "                if not normalise:\n",
    "                    plot_maximum(clustersinfo,cluster,label,level, directory,algorithm=algorithm)\n",
    "                    plot_maximum_size(clustersinfo,label,level, directory,algorithm=algorithm)\n",
    "                    plot_maximum_label(clustersinfo,label,level, directory,algorithm=algorithm)\n",
    "                    plot_sizes(level,directory, algorithm=algorithm)\n",
    "            except:\n",
    "                print(sys.exc_info()[0])\n",
    "            try:\n",
    "                fraction_sites_shuffle = get_fraction_sites(cluster, pd.read_csv(\"%s/files.dat.shuf\"%directory, index_col=[0]),label=label, normalise=normalise)\n",
    "                clustersinfo_shuffle = get_clustersinfo(cluster, fraction_sites_shuffle)\n",
    "                plot_cluster_composition(fraction_sites_shuffle,directory,level, label=label, shuffled=True, normalise=normalise)\n",
    "                if not normalise:\n",
    "                    plot_maximum(clustersinfo,cluster,label,level,directory,clustersinfo_shuffle,algorithm=algorithm)\n",
    "                    plot_maximum_size(clustersinfo,label,level, directory,clustersinfo_shuffle,algorithm=algorithm)\n",
    "                    plot_maximum_label(clustersinfo,label,level, directory,clustersinfo_shuffle,algorithm=algorithm)\n",
    "                    plot_labels_size(clustersinfo,label,level, directory,clustersinfo_shuffle,algorithm=algorithm)\n",
    "            except:\n",
    "                print(\"must shuffle files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##useful for R clustree plot\n",
    "def reindex(x):\n",
    "    i=1\n",
    "    last=x[0]\n",
    "    new = []\n",
    "    for xi in x:\n",
    "        if xi == last:\n",
    "            pass\n",
    "        else:\n",
    "            i+=1\n",
    "            last=xi\n",
    "        new.append(i)\n",
    "    return new\n",
    "\n",
    "df_clusters = pd.read_csv(\"%s/%s/%s_level_%d_clusters.csv\"%(directory,algorithm,algorithm,L), header=[0])\n",
    "df_labels = pd.DataFrame()\n",
    "shape = df_clusters.dropna().shape\n",
    "files = df_clusters.dropna().values.reshape(shape[0]*shape[1],)\n",
    "for level in np.arange(L+1)[::-1]:\n",
    "    print(level)\n",
    "    df_clusters = pd.read_csv(\"%s/%s/%s_level_%d_clusters.csv\"%(directory,algorithm,algorithm,level), header=[0])\n",
    "    currentlevellabels = []\n",
    "    for file in files:\n",
    "        s = df_clusters[df_clusters.isin([file])].any(0)\n",
    "        a = s.index[s]\n",
    "        currentlevellabels.append(int(a[0][8:])-1)\n",
    "    df_labels.insert(0,'l%d'%level,currentlevellabels)\n",
    "    del currentlevellabels\n",
    "filelabels = []\n",
    "filesublabels = []\n",
    "for file in files:\n",
    "    filelabels.append(get_file(file, df_files)[labels[0]])\n",
    "    filesublabels.append(get_file(file, df_files)[labels[1]])\n",
    "df_labels.insert(0,'tissue', filelabels)\n",
    "df_labels.insert(0,'subtissue', filesublabels)\n",
    "df_labels.sort_values(by=['tissue','subtissue'], inplace=True)\n",
    "df_labels.sort_values(by=[\"l%d\"%l for l in np.arange(L+1)[::-1]], axis=0, inplace=True)\n",
    "filelabels = df_labels['tissue']\n",
    "filesublabels = df_labels['subtissue']\n",
    "df_labels = df_labels.apply(reindex, axis=0)\n",
    "df_labels['tissue']=filelabels\n",
    "df_labels['subtissue']=filesublabels\n",
    "df_labels.to_csv(\"%s/%s/topsbm_labels.csv\"%(directory,algorithm), index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    xl = getclustersizesarray(directory, L)\n",
    "    with open(\"%s/clustersizes.txt\" % directory, 'w') as f:\n",
    "        for x in xl:\n",
    "            f.write(\"%d\\n\" % x)\n",
    "except:\n",
    "    print(\"cannot save clustersizes.txt\")\n",
    "\n",
    "try:\n",
    "    tl = gettopicsizesarray(directory, L)\n",
    "    with open(\"%s/topicsizes.txt\" % directory, 'w') as f:\n",
    "        for x in tl:\n",
    "            f.write(\"%d\\n\" % x)\n",
    "except:\n",
    "    print(\"cannot save topicsizes.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_scores(directory, labels, l=L, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    xl = [len(get_cluster_given_l(li,directory)) for li in np.linspace(0,L,L+1)]\n",
    "except:\n",
    "    xl = [2,10,100,500]\n",
    "with open(\"%s/clustersizes.txt\"%directory,'w') as f:\n",
    "    for x in xl:\n",
    "        f.write(\"%d\\n\"%x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12,8))\n",
    "ax = fig.subplots(1)\n",
    "#add_score_lines(ax,scores,[labels[0]],xl, h=True, c=True, alpha=1)\n",
    "h = np.array(scores['hsbm']['h'])\n",
    "c = np.array(scores['hsbm']['c'])\n",
    "ax.plot(xl,h, marker='x', ls='--', lw=2, label='homogeneity')\n",
    "ax.plot(xl,c, marker='x', ls='-.', lw=2,label='completeness')\n",
    "ax.plot(xl,scores['hsbm']['V'], marker='x', ls='-', lw=2, label='score')\n",
    "ax.set_xscale('log')\n",
    "#ax.plot(xl, 2*h*c/(h+c), ls='-',c='g')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(fontsize=25)\n",
    "plt.xlabel('number of clusters', fontsize=20)\n",
    "plt.ylabel('measure', fontsize=20)\n",
    "plt.show()\n",
    "fig.savefig(\"%s/metric_scores_primarysite.pdf\"%(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12,8))\n",
    "ax = fig.subplots(1)\n",
    "add_score_lines(ax,scores,labels,xl)\n",
    "#h = np.array(scores['primary_site']['h'])\n",
    "#c = np.array(scores['secondary_site']['c'])\n",
    "#ax.plot(xl, 2*h*c/(h+c), ls='-',c='g')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()\n",
    "fig.savefig(\"%s/metric_scores.pdf\"%(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.array(scores[labels[0]]['h'])\n",
    "c = np.array(scores[labels[1]]['c'])\n",
    "scores['hSBM'] = {\n",
    "    'h':h,\n",
    "    'c':c,\n",
    "    'V':2*h*c/(h+c)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['hierarchical'] = pd.read_csv(\"%s/hierarchical.scores\"%directory).to_dict(orient='list')\n",
    "scores['lda'] = pd.read_csv(\"%s/lda.scores\"%directory).to_dict(orient='list')\n",
    "#scores['hsbm->hierachical'] = pd.read_csv(\"%s/hierhsbm.scores\"%directory).to_dict(orient='list')\n",
    "scores['hsbm'] = scores['primary_site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['lda'] = pd.read_csv(\"%s/lda.scores\"%directory).to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['hsbm'] = scores['primary_site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12,8))\n",
    "ax = fig.subplots(1)\n",
    "add_score_lines(ax,scores,labels=['hsbm', 'lda', 'hierarchical', 'shuffle'],xl=xl)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()\n",
    "fig.savefig(\"%s/metric_scores_all.pdf\"%(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"%s/mainTable.csv\"%directory, delimiter=',', index_col=0).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.parallel_apply(lambda x: x/np.sum(x).astype(float), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes = {}\n",
    "algorithms = ['topsbm','lda', 'hierarchical', 'hierhsbm']\n",
    "for algorithm in algorithms:\n",
    "    print(algorithm)\n",
    "    silhouettes[algorithm]=[]\n",
    "    for level in np.arange(L+1)[::-1]:\n",
    "        cluster = get_cluster_given_l(level, directory, algorithm=algorithm)\n",
    "        predicted_labels = []\n",
    "        for c in cluster:\n",
    "            for sample in cluster[c]:\n",
    "                try:\n",
    "                    predicted_labels.append([get_file(sample,df_files).name,c])\n",
    "                except:\n",
    "                    print(sys.exc_info()[0])\n",
    "                    print(\"error searching %s in %s\" % (label,sample))\n",
    "        predicted_labels=np.array(predicted_labels)\n",
    "        labs = np.array([l[1] for l in predicted_labels if l[0] in X.index])\n",
    "        X_values = X[X.index.isin([l for l in predicted_labels.T[0] if l in X.index])].values\n",
    "        try:\n",
    "            silhouettes[algorithm].append((metrics.silhouette_score(X_values, labs, metric='correlation')))\n",
    "        except:\n",
    "            print(sys.exc_info()[1])\n",
    "            silhouettes[algorithm].append(np.nan)\n",
    "silhouettes['hsbm']=silhouettes['topsbm']\n",
    "silhouettes.pop('topsbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(9,6))\n",
    "for algorithm in ['hsbm', 'lda', 'hierarchical','hierhsbm']:\n",
    "    plt.plot(xl[::-1],np.array(silhouettes[algorithm]), lw=2, ms=14, marker='x', label=algorithm)\n",
    "plt.ylabel(\"Silhouette\", fontsize=20)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=20)\n",
    "plt.title(\"Silhouette score [correlation]\", fontsize=20)\n",
    "plt.ylim((-1.1,1.1))\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='best', fontsize=20)\n",
    "plt.show()\n",
    "fig.savefig(\"%s/silhouette.pdf\"%directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "https://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for c in cluster:\n",
    "    print(c)\n",
    "    for sample in cluster[c]:\n",
    "        #true_labels.append(getFile(sample)['primary_site'].values[0])\n",
    "        try:\n",
    "            true_labels.append(df_files.loc[sample][label])\n",
    "            predicted_labels.append(c)\n",
    "        except:\n",
    "            print(\"error in %s\"%sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, labels_true = np.unique(true_labels,return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_metrics=pd.Series(name=setup+'__level_%d_%s'%(level,label))\n",
    "series_metrics['MI']=metrics.cluster.normalized_mutual_info_score(labels_true, predicted_labels, average_method='arithmetic')\n",
    "series_metrics['Homogeneity']=metrics.cluster.homogeneity_score(labels_true, predicted_labels) \n",
    "series_metrics['Completness']=metrics.cluster.completeness_score(labels_true, predicted_labels) \n",
    "series_metrics['Vmeasure']=metrics.cluster.v_measure_score(labels_true, predicted_labels)\n",
    "series_metrics['Fowlkes-Mallows']=metrics.cluster.fowlkes_mallows_score(labels_true,predicted_labels)\n",
    "series_metrics['AdjustedRandIndex']=metrics.cluster.adjusted_rand_score(labels_true,predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_metrics = pd.DataFrame(columns=['AdjustedRandIndex','MI','Homogeneity','Completness','Vmeasure','Fowlkes-Mallows'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_metrics.append(series_metrics, ignore_index=False, verify_integrity=True).to_csv(\"results/hSBM/metrics.csv\", index=True,header=True)\n",
    "except:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.read_csv(\"results/hSBM/metrics.csv\", index_col=[0],header=[0])\n",
    "df_metrics.sort_values(by=['MI','Homogeneity'], ascending=False, axis=0, inplace=True)\n",
    "df_metrics.round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 20)) # set size frame\n",
    "ax.xaxis.set_visible(False)  # hide the x axis\n",
    "ax.yaxis.set_visible(False)  # hide the y axis\n",
    "ax.set_frame_on(False)  # no visible frame, uncomment if size is ok\n",
    "tabla = pd.plotting.table(ax, df_metrics.round(decimals=2), loc='upper right', colWidths=[0.12]*len(df_metrics.columns))  # where df is your data frame\n",
    "tabla.auto_set_font_size(False) # Activate set fontsize manually\n",
    "tabla.set_fontsize(12) # if ++fontsize is necessary ++colWidths\n",
    "tabla.scale(1, 2) # change size table\n",
    "plt.savefig('results/hSBM/metrics.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.cluster.contingency_matrix(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files[df_files['primary_site']=='Brain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
